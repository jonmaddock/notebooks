{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrange multipliers\n",
    "\n",
    "I need to check that I'm working out Lagrange multipliers correctly, so that I can compare convergence parameters.\n",
    "\n",
    "Choose a test case with difference numbers of optimisation parameters and constraints, for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCP Error\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "# Set up the problem and define the expected result.\n",
    "# Maximise f(x1,x2) = x1 + x2, subject to the following constraint:\n",
    "# c1(x1,x2) = x1**2 + x2**2 - 1 = 0\n",
    "# http://en.wikipedia.org/wiki/Lagrange_multiplier\n",
    "m = 1\n",
    "n = 2\n",
    "# case.solver_args.x[0:2] = 1.0e0\n",
    "# case.solver_args.maximise = True\n",
    "\n",
    "# N.B. results can flip to minimum instead of maximum\n",
    "# if x(1), x(2) are initialised at different points...\n",
    "\n",
    "# Construct problem\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Maximize(x[0] + x[1])\n",
    "constraints = [x[0]**2 == 0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve with cvxpy to get Lagrange multipliers out\n",
    "try:\n",
    "    result = prob.solve()\n",
    "    print(x.value)\n",
    "except cp.DCPError:\n",
    "    print(\"DCP Error\")\n",
    "\n",
    "# Expected values\n",
    "# x_exp = np.array([0.5 * 2.0 ** (1 / 2), 0.5 * 2.0 ** (1 / 2)])\n",
    "# objf_exp = 2.0 ** (1 / 2)\n",
    "# c_exp = np.array([0.0])\n",
    "# vlam_exp = np.array([1.0 * 2.0 ** (1 / 2)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that test case 4 isn't convex, so `cvxpy` can't handle it. Interesting...\n",
    "\n",
    "Try test case 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrange multipliers:\n",
      "1.594491808182122\n",
      "1.8465902435562314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "# Minimise f(x1,x2) = (x1 - 2)**2 + (x2 - 1)**2 subject to the following constraints:\n",
    "# c1(x1,x2) = x1 - 2*x2 + 1 = 0\n",
    "# c2(x1,x2) = -x1**2/4 - x2**2 + 1 >= 0\n",
    "n = 2\n",
    "m = 2\n",
    "\n",
    "# Construct problem\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize((x[0] - 2)**2 + (x[1] - 1)**2)\n",
    "constraints = [x[0] - 2*x[1] + 1 == 0, -x[0]**2/4 - x[1]**2 + 1 >= 0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve with cvxpy to get Lagrange multipliers out\n",
    "result = prob.solve()\n",
    "\n",
    "# Expected values\n",
    "x_exp = np.array([8.228756e-1, 9.114378e-1])\n",
    "objf_exp = 1.393464\n",
    "vlam_exp = np.array([-1.594491, 1.846591])\n",
    "\n",
    "# Assert we've got the expected values (same as VMCON)\n",
    "np.testing.assert_allclose(x.value, x_exp)\n",
    "np.testing.assert_allclose(prob.value, objf_exp, rtol=1e-6)\n",
    "\n",
    "print(\"Lagrange multipliers:\")\n",
    "lag_mults = []\n",
    "for constraint in constraints:\n",
    "    lag_mults.append(constraint.dual_value)\n",
    "    print(f\"{constraint.dual_value}\")\n",
    "\n",
    "# Sign flipped on first value! Strange...\n",
    "# np.testing.assert_allclose(lag_mults, vlam_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cvxpy solves, and the solution is the same as VMCON. The Lagrange multipliers are almost the same, with the first Lagrange multiplier differing in sign.\n",
    "\n",
    "Now use cvxpy's solution to calculate my own Lagrange multipliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lagrange multipliers\n",
      "[-1.59449112  1.84659144]\n"
     ]
    }
   ],
   "source": [
    "# Now try alternative lag mults calc\n",
    "def calc_lag_mults(cnorm, fgrd, m):\n",
    "    n = fgrd.shape[0]\n",
    "    constr_partials = cnorm\n",
    "    # Solve n (no. of optimisation parameters) simultaneous equations to determine m (no. of constraints) Lagrange multipliers\n",
    "    lag_mults = np.linalg.lstsq(constr_partials[:n, :m], fgrd, rcond=None)\n",
    "\n",
    "    # Return solution vector only\n",
    "    return lag_mults[0]\n",
    "\n",
    "def eval_grad(x):\n",
    "    \"\"\"Gradient function evaluator.\"\"\"\n",
    "\n",
    "    fgrd = np.array([2.0 * (x[0] - 2.0), 2.0 * (x[1] - 1.0)])\n",
    "    cnorm = np.array([[1.0, -0.5 * x[0]], [-2.0, -2.0 * x[1]]])\n",
    "    return fgrd, cnorm\n",
    "\n",
    "fgrd, cnorm = eval_grad(x.value)\n",
    "my_lag_mults = calc_lag_mults(cnorm, fgrd, m)\n",
    "print(\"My Lagrange multipliers\")\n",
    "print(my_lag_mults)\n",
    "\n",
    "np.testing.assert_allclose(my_lag_mults, vlam_exp, rtol=1e-6)\n",
    "# Fails due to first Lag mult sign diff\n",
    "# np.testing.assert_allclose(my_lag_mults, lag_mults, rtol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Lagrange multipliers from VMCON, cvxpy and my own all have the same absolute value. However, the first element of the cvxpy answer has a different sign. This doesn't bother me that much: I'm interested in replicating VMCON's Lagrange multipliers using my own calculation. In this case, it works. So, why is there a difference between the test case and a full run?\n",
    "\n",
    "## Add a bound\n",
    "\n",
    "My suspicion is that the addition of bounds are causing the difference. The integration test cases have no bounds, whereas the regression tests do; that could explain the similarity of Lagrange multipliers in the integration case, but the difference in the regression case. To test this, I'll add a bound to a integration case.\n",
    "\n",
    "Add a bound to integration test case 1, with cvxpy. This has to be expressed as a constraint for cvxpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrange multipliers:\n",
      "1.594491819648742\n",
      "1.846591297358343\n",
      "2.3919474947237467e-10\n"
     ]
    }
   ],
   "source": [
    "# Minimise f(x1,x2) = (x1 - 2)**2 + (x2 - 1)**2 subject to the following constraints:\n",
    "# c1(x1,x2) = x1 - 2*x2 + 1 = 0\n",
    "# c2(x1,x2) = -x1**2/4 - x2**2 + 1 >= 0\n",
    "# Add upper bound to x[0]: x[0] <= 1\n",
    "# 1 - x[0] >= 0\n",
    "n = 2\n",
    "m = 3\n",
    "\n",
    "# Construct problem\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize((x[0] - 2)**2 + (x[1] - 1)**2)\n",
    "constraints = [x[0] - 2*x[1] + 1 == 0, -x[0]**2/4 - x[1]**2 + 1 >= 0, x[0] >= 0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve with cvxpy to get Lagrange multipliers out\n",
    "result = prob.solve()\n",
    "\n",
    "# Expected values\n",
    "x_exp = np.array([8.228756e-1, 9.114378e-1])\n",
    "objf_exp = 1.393464\n",
    "vlam_exp = np.array([-1.594491, 1.846591])\n",
    "\n",
    "# Assert we've got the expected values (same as VMCON)\n",
    "np.testing.assert_allclose(x.value, x_exp)\n",
    "np.testing.assert_allclose(prob.value, objf_exp, rtol=1e-6)\n",
    "\n",
    "print(\"Lagrange multipliers:\")\n",
    "lag_mults = []\n",
    "for constraint in constraints:\n",
    "    lag_mults.append(constraint.dual_value)\n",
    "    print(f\"{constraint.dual_value}\")\n",
    "\n",
    "# Sign flipped on first value! Strange...\n",
    "# np.testing.assert_allclose(lag_mults, vlam_exp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bound was chosen so as not to affect the solution (it's not \"active\"). However, we now get 3 Lagrange multipliers. Using my existing calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lagrange multipliers\n",
      "[-0.75454046  0.92502484  1.21911802]\n"
     ]
    }
   ],
   "source": [
    "def eval_grad_with_bound(x):\n",
    "    \"\"\"Gradient function evaluator.\"\"\"\n",
    "\n",
    "    fgrd = np.array([2.0 * (x[0] - 2.0), 2.0 * (x[1] - 1.0)])\n",
    "    # Add partial differentials for bound on x[0] to cnorm\n",
    "    cnorm = np.array([[1.0, -0.5 * x[0], -1], [-2.0, -2.0 * x[1], 0]])\n",
    "    return fgrd, cnorm\n",
    "\n",
    "fgrd, cnorm = eval_grad_with_bound(x.value)\n",
    "my_lag_mults = calc_lag_mults(cnorm, fgrd, m)\n",
    "print(\"My Lagrange multipliers\")\n",
    "print(my_lag_mults)\n",
    "\n",
    "# np.testing.assert_allclose(my_lag_mults, vlam_exp, rtol=1e-6)\n",
    "# Fails due to first Lag mult sign diff\n",
    "# np.testing.assert_allclose(my_lag_mults, lag_mults, rtol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that adding in an inactive bound causes the cvxpy and my Lagrange multipliers to go from completely agreeing to completely disagreeing.\n",
    "\n",
    "## Adding an active bound\n",
    "\n",
    "If a constraint is added `x[0] <= 0` that is binding (i.e. it changes the solution), what happens in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrange multipliers:\n",
      "-0.4999997899850783\n",
      "7.431496512849663e-09\n",
      "4.500009171412716\n"
     ]
    }
   ],
   "source": [
    "# Minimise f(x1,x2) = (x1 - 2)**2 + (x2 - 1)**2 subject to the following constraints:\n",
    "# c1(x1,x2) = x1 - 2*x2 + 1 = 0\n",
    "# c2(x1,x2) = -x1**2/4 - x2**2 + 1 >= 0\n",
    "# Add upper bound to x[0]: x[0] <= 0\n",
    "n = 2\n",
    "m = 3\n",
    "\n",
    "# Construct problem\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize((x[0] - 2)**2 + (x[1] - 1)**2)\n",
    "constraints = [x[0] - 2*x[1] + 1 == 0, -x[0]**2/4 - x[1]**2 + 1 >= 0, x[0] <= 0]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve with cvxpy to get Lagrange multipliers out\n",
    "result = prob.solve()\n",
    "\n",
    "print(\"Lagrange multipliers:\")\n",
    "lag_mults = []\n",
    "for constraint in constraints:\n",
    "    lag_mults.append(constraint.dual_value)\n",
    "    print(f\"{constraint.dual_value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bound was chosen to deliberately affect the solution (it's \"active\"). This has changed all 3 Lagrange multipliers, with the 2nd being very small. I suspect that the 2nd multiplier now corresponds to an \"inactive\" constraint, so I try removing it from the Lagrange multiplier calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lagrange multipliers\n",
      "[0.5 4.5]\n"
     ]
    }
   ],
   "source": [
    "def eval_grad_with_bound(x):\n",
    "    \"\"\"Gradient function evaluator.\"\"\"\n",
    "\n",
    "    # Same as before\n",
    "    fgrd = np.array([2.0 * (x[0] - 2.0), 2.0 * (x[1] - 1.0)])\n",
    "\n",
    "    # Only include partial differentials constraint 1 and the bound (drop 2nd constraint)\n",
    "    cnorm = np.array([[1.0, -1], [-2.0, 0]])\n",
    "    return fgrd, cnorm\n",
    "\n",
    "fgrd, cnorm = eval_grad_with_bound(x.value)\n",
    "my_lag_mults = calc_lag_mults(cnorm, fgrd, m-1)\n",
    "print(\"My Lagrange multipliers\")\n",
    "print(my_lag_mults)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the [0] and [2] terms of the cvxpy Lagrange multipliers now equal the two multipliers produced by the independent calculation. In other words, it appears that by \"dropping\" the inactive constraints when calculating the Lagrange multipliers causes the cvxpy and independent calculations to align. Trying a different constraint to change what's active will validate this theory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lagrange multipliers:\n",
      "3.0742856194040384e-10\n",
      "1.2941223487459272\n",
      "0.7647103085548425\n"
     ]
    }
   ],
   "source": [
    "# Minimise f(x1,x2) = (x1 - 2)**2 + (x2 - 1)**2 subject to the following constraints:\n",
    "# c1(x1,x2) = x1 - 2*x2 + 1 = 0\n",
    "# c2(x1,x2) = -x1**2/4 - x2**2 + 1 >= 0\n",
    "# Add lower bound to x[0]: x[0] >= 1.8\n",
    "n = 2\n",
    "m = 3\n",
    "\n",
    "# Construct problem\n",
    "x = cp.Variable(n)\n",
    "objective = cp.Minimize((x[0] - 2)**2 + (x[1] - 1)**2)\n",
    "constraints = [x[0] - 2*x[1] + 1 >= 0, -x[0]**2/4 - x[1]**2 + 1 >= 0, x[0] >= 1.8]\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Solve with cvxpy to get Lagrange multipliers out\n",
    "result = prob.solve()\n",
    "\n",
    "print(\"Lagrange multipliers:\")\n",
    "lag_mults = []\n",
    "for constraint in constraints:\n",
    "    lag_mults.append(constraint.dual_value)\n",
    "    print(f\"{constraint.dual_value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a bound of `x[0] >= 1.8`, it looks like only multipliers [1] and [2] are definitely non-zero: it looks like constraint [0] is inactive. Removing constraint [0] from the independent calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Lagrange multipliers\n",
      "[1.29415734 0.76474161]\n"
     ]
    }
   ],
   "source": [
    "def eval_grad_with_bound(x):\n",
    "    \"\"\"Gradient function evaluator.\"\"\"\n",
    "\n",
    "    # Same as before\n",
    "    fgrd = np.array([2.0 * (x[0] - 2.0), 2.0 * (x[1] - 1.0)])\n",
    "\n",
    "    # Only include partial differentials for constraint [1] and bound [0]\n",
    "    # Drop constraint [0]\n",
    "    cnorm = np.array([[-0.5 * x[0], 1], [-2.0 * x[1], 0]])\n",
    "    return fgrd, cnorm\n",
    "\n",
    "fgrd, cnorm = eval_grad_with_bound(x.value)\n",
    "my_lag_mults = calc_lag_mults(cnorm, fgrd, m-1)\n",
    "print(\"My Lagrange multipliers\")\n",
    "print(my_lag_mults)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, once the inactive constraint was excluded from the independent calculation (excluded from `cnorm`), the Lagrange multipliers agree.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "To get the correct, identical Lagrange multipliers from VMCON, cvxpy and the independent calculation, the inactive constraints and bounds need to be excluded from each. For the independent calculation, this means excluding inactive bounds terms from the least-squares solution of the simultaneous linear equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95e8614a6e18ad6e528160ac32f08bcfa19db99daf3816cbd89c3976c3924301"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
